{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7de25cb5-fef3-4e29-8cfc-8cf956e280ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import cv2\n",
    "import nibabel as nib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from skimage.transform import resize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "import scipy.ndimage\n",
    "from skimage.measure import marching_cubes\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "\n",
    "# Function to load NIfTI files with memory mapping\n",
    "def load_nifti_memmap(file_path):\n",
    "    img = nib.load(file_path)\n",
    "    data = img.get_fdata(dtype=np.float32, caching='unchanged')  # Memory-mapped array\n",
    "    affine = img.affine\n",
    "    header = img.header\n",
    "    return data, affine, header\n",
    "\n",
    "# Generator function to load data in batches\n",
    "def data_generator(file_list, data_path, mask_path, batch_size, target_shape=None):\n",
    "    while True:\n",
    "        np.random.shuffle(file_list)\n",
    "        for start in range(0, len(file_list), batch_size):\n",
    "            end = min(start + batch_size, len(file_list))\n",
    "            batch_files = file_list[start:end]\n",
    "            \n",
    "            X_batch = []\n",
    "            y_batch = []\n",
    "            \n",
    "            for filename in batch_files:\n",
    "                img_path = os.path.join(data_path, filename)\n",
    "                corresponding_mask_path = os.path.join(mask_path, filename)\n",
    "                \n",
    "                image, _, _ = load_nifti_memmap(img_path)\n",
    "                mask, _, _ = load_nifti_memmap(corresponding_mask_path)\n",
    "                \n",
    "                # Ensure image and mask have the same shape (and possibly resize if needed)\n",
    "                if target_shape:\n",
    "                    image = resize_volume(image, target_shape)\n",
    "                    mask = resize_volume(mask, target_shape)\n",
    "                \n",
    "                X_batch.append(image)\n",
    "                y_batch.append(mask)\n",
    "            \n",
    "            X_batch = np.array(X_batch)[..., np.newaxis]  # Adding channel dimension\n",
    "            y_batch = np.array(y_batch)[..., np.newaxis]  # Adding channel dimension\n",
    "            \n",
    "            yield X_batch, y_batch\n",
    "\n",
    "# Function to resize volumes (if needed)\n",
    "def resize_volume(img, target_shape):\n",
    "    current_shape = img.shape\n",
    "    if current_shape == target_shape:\n",
    "        return img\n",
    "    # Example: using scipy for interpolation\n",
    "    resized_img = scipy.ndimage.zoom(img, (target_shape[0]/current_shape[0], target_shape[1]/current_shape[1], target_shape[2]/current_shape[2]), order=3)\n",
    "    return resized_img\n",
    "\n",
    "def resize_image(image, target_shape):\n",
    "    # Resize the image to match the target shape (height, width)\n",
    "    return resize(image, target_shape, preserve_range=True, anti_aliasing=True)\n",
    "\n",
    "def pad_or_crop_volume(volume, target_shape):\n",
    "    current_shape = volume.shape\n",
    "    \n",
    "    # Calculate padding width for each dimension\n",
    "    pad_width = [(0, max(target_shape[i] - current_shape[i], 0)) for i in range(3)]\n",
    "    \n",
    "    # Pad the volume to the target shape\n",
    "    volume = np.pad(volume, pad_width, mode='constant', constant_values=0)\n",
    "    \n",
    "    # Calculate cropping dimensions for each dimension\n",
    "    crop_start = [(volume.shape[i] - target_shape[i]) // 2 for i in range(3)]\n",
    "    crop_end = [crop_start[i] + target_shape[i] for i in range(3)]\n",
    "    \n",
    "    # Crop the volume to the target shape\n",
    "    slices = [slice(crop_start[i], crop_end[i]) for i in range(3)]\n",
    "    volume = volume[slices[0], slices[1], slices[2]]\n",
    "    \n",
    "    return volume\n",
    "\n",
    "def calculate_volume(mask, voxel_volume):\n",
    "    # Calculate the volume based on the mask\n",
    "    return np.sum(mask) * voxel_volume\n",
    "    \n",
    "def unet_3d(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Downsampling\n",
    "    c1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c1)\n",
    "    c1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c1)  # Added layer\n",
    "    c1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c1)  # Added layer\n",
    "    p1 = MaxPooling3D((2, 2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c2)\n",
    "    c2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c2)  # Added layer\n",
    "    c2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c2)  # Added layer\n",
    "    p2 = MaxPooling3D((2, 2, 2))(c2)\n",
    "    \n",
    "    c3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(c3)\n",
    "    c3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(c3)  # Added layer\n",
    "    c3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(c3)  # Added layer\n",
    "    \n",
    "    # Upsampling\n",
    "    u4 = UpSampling3D((2, 2, 2))(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(u4)\n",
    "    c4 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c4)\n",
    "    c4 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c4)  # Added layer\n",
    "    c4 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c4)  # Added layer\n",
    "    \n",
    "    u5 = UpSampling3D((2, 2, 2))(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(u5)\n",
    "    c5 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c5)\n",
    "    c5 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c5)  # Added layer\n",
    "    c5 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c5)  # Added layer\n",
    "    \n",
    "    outputs = Conv3D(1, (1, 1, 1), activation='sigmoid')(c5)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    #model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy', dice_coefficient, jaccard_index])\n",
    "    return model\n",
    "\n",
    "# Dice coefficient\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + 1)\n",
    "\n",
    "# Jaccard index\n",
    "def jaccard_index(y_true, y_pred):\n",
    "    y_true_f = tf.keras.backend.flatten(y_true)\n",
    "    y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "    intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection + 1)\n",
    "\n",
    "# Example paths (update these to your dataset paths)\n",
    "data_path = r'/home/icmr/Downloads/niigz dicom/Arterial Phase'  # Update with the correct path\n",
    "mask_path = r'/home/icmr/Downloads/niigz liver/Arterial Phase'  # Update with the correct path\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Data path not found: {data_path}\")\n",
    "if not os.path.exists(mask_path):\n",
    "    raise FileNotFoundError(f\"Mask path not found: {mask_path}\")\n",
    "\n",
    "# List of files\n",
    "file_list = [filename for filename in os.listdir(data_path) if filename.endswith('.nii.gz')]\n",
    "\n",
    "# Split the dataset\n",
    "train_val_files, test_files = train_test_split(file_list, test_size=0.2, random_state=42)\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
    "\n",
    "# Define and compile the model\n",
    "input_shape = (128, 128, 64, 1)  # Example input shape, adjust accordingly\n",
    "model = unet_3d(input_shape)\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy', dice_coefficient, jaccard_index])\n",
    "  \n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('unet3d_best_model.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create data generators\n",
    "train_generator = data_generator(train_files, data_path, mask_path, batch_size, target_shape=input_shape[:3])\n",
    "val_generator = data_generator(val_files, data_path, mask_path, batch_size, target_shape=input_shape[:3])\n",
    "test_generator = data_generator(test_files, data_path, mask_path, batch_size, target_shape=input_shape[:3])\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_files) // batch_size\n",
    "validation_steps = len(val_files) // batch_size\n",
    "test_steps = len(test_files) // batch_size\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=50, steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, callbacks=[checkpoint])\n",
    "\n",
    "# Save the model\n",
    "model.save('unet3d_model.h5')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy, test_dice, test_jaccard = model.evaluate(test_generator, steps=test_steps)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Dice Coefficient: {test_dice}\")\n",
    "print(f\"Test Jaccard Index: {test_jaccard}\")\n",
    "\n",
    "\n",
    "def display_results_with_marching_cubes(model, file_list, data_path, mask_path, num_images=3):\n",
    "    target_shape = (128, 128, 64)  # Fixed shape for all volumes\n",
    "    num_images = min(num_images, len(file_list))  # Ensure we do not exceed the number of files\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Load image and mask\n",
    "        image_file_path = os.path.join(data_path, file_list[i])\n",
    "        mask_file_path = os.path.join(mask_path, file_list[i])\n",
    "        \n",
    "        print(f\"Processing file {i+1}/{num_images}: {file_list[i]}\")\n",
    "        \n",
    "        try:\n",
    "            image = nib.load(image_file_path).get_fdata(dtype=np.float32)\n",
    "            mask = nib.load(mask_file_path).get_fdata(dtype=np.float32)\n",
    "            \n",
    "            # Resize image and mask to expected shape\n",
    "            image = resize_image(image, target_shape[:2])\n",
    "            mask = resize_image(mask, target_shape[:2])\n",
    "            \n",
    "            # Pad or crop image and mask to match target depth\n",
    "            image = pad_or_crop_volume(image, target_shape)\n",
    "            mask = pad_or_crop_volume(mask, target_shape)\n",
    "            \n",
    "            # Preprocess image and mask\n",
    "            image = image[..., np.newaxis]\n",
    "            mask = mask[..., np.newaxis]\n",
    "\n",
    "            # Get prediction\n",
    "            prediction = model.predict(np.expand_dims(image, axis=0))[0, ..., 0]\n",
    "            \n",
    "            # Calculate the volume\n",
    "            voxel_volume = np.prod(image.shape) / np.sum(mask > 0)  # Example volume calculation\n",
    "            liver_volume = calculate_volume(mask, voxel_volume)\n",
    "            print(f\"Calculated volume for {file_list[i]}: {liver_volume:.2f} cubic units\")\n",
    "\n",
    "            # 3D Visualization using marching cubes\n",
    "            verts, faces, _, _ = marching_cubes(mask[..., 0], level=0.5)\n",
    "            fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # Subplot 1: Original Data (image slice)\n",
    "            ax1 = fig.add_subplot(131)\n",
    "            ax1.imshow(image[:, :, target_shape[2] // 2, 0], cmap='gray')\n",
    "            ax1.set_title(\"Image Slice\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            # Subplot 2: Ground Truth Mask (slice)\n",
    "            ax2 = fig.add_subplot(132)\n",
    "            ax2.imshow(mask[:, :, target_shape[2] // 2, 0], cmap='gray')\n",
    "            ax2.set_title(\"Mask Slice\")\n",
    "            ax2.axis('off')\n",
    "\n",
    "            # Subplot 3: 3D Reconstructed Surface\n",
    "            ax3 = fig.add_subplot(133, projection='3d')\n",
    "            mesh = Poly3DCollection(verts[faces], alpha=0.7, edgecolor='none')\n",
    "            ax3.add_collection3d(mesh)\n",
    "            ax3.set_xlim(0, mask.shape[0])\n",
    "            ax3.set_ylim(0, mask.shape[1])\n",
    "            ax3.set_zlim(0, mask.shape[2])\n",
    "            ax3.set_title(\"3D Reconstructed Surface\")\n",
    "            ax3.set_xlabel(\"X axis\")\n",
    "            ax3.set_ylabel(\"Y axis\")\n",
    "            ax3.set_zlabel(\"Z axis\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_list[i]}: {e}\")\n",
    "\n",
    "# Call the updated function\n",
    "display_results_with_marching_cubes(model, val_files, data_path, mask_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
